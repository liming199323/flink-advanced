# 示例程序
[Batch.scala](https://github.com/GourdErwa/flink-advanced/blob/master/src/main/scala/io/gourd/flink/scala/games/batch/Batch.scala)
```java
// 操作原始 DataSet API 完成 2个表数据过滤join聚合操作
object BatchDataSet extends BatchExecutionEnvironmentApp {

  // 用户登录数据 DataSet
  val userLoginDataSet = DataSet.userLogin(this)
  // 角色登录数据 DataSet
  val roleLoginDataSet = DataSet.roleLogin(this)

  userLoginDataSet
    .filter(_.dataUnix > 1571414499)
    .filter(_.status == "LOGIN")
    .join(roleLoginDataSet, JoinHint.BROADCAST_HASH_FIRST).where(_.uid).equalTo(_.uid)
    .apply((left, _) => left.platform -> 1)
    .groupBy(0)
    .sum(1)
    .sortPartition(1, Order.ASCENDING)
    .print()
}

// 操作 Table API 完成 2个表数据过滤join聚合操作
object BatchTable extends BatchTableEnvironmentApp {

  private val userLogin = RegisterDataSet.userLogin(this)
  private val roleLogin = RegisterDataSet.roleLogin(this)

  btEnv.scan(userLogin)
    .select("platform,dataUnix,uid,status")
    .where('dataUnix > 1571414499 && 'status === "LOGIN")
    .join(btEnv.scan(roleLogin).select("uid as r_uid"), "uid = r_uid")
    .groupBy("platform")
    .select("platform as p , count(platform) as c")
    .orderBy('c.asc)
    .toDataSet[(String, Long)]
    .print()
}

// 操作 SQL 完成 2个表数据过滤join聚合操作
object BatchSQL extends BatchTableEnvironmentApp {

  private val table = RegisterDataSet.userLogin(this)

  btEnv.sqlQuery(
    s"""
       |SELECT platform AS p,COUNT(platform) AS c FROM
       |(
       |SELECT platform,dataUnix,uid,status FROM $table
       |WHERE dataUnix > 0 AND status = 'LOGIN'
       |)
       |GROUP BY platform
       |""".stripMargin)
    .toDataSet[(String, Long)]
    .print()
}
```
# 程序数据源输入（Data Sources）
## 基于文件：
- `readTextFile(path)// TextInputFormat`-逐行读取文件，并将它们作为字符串返回。
- `readTextFileWithValue(path)// TextValueInputFormat`-逐行读取文件，并将它们作为StringValues返回。StringValues 是可变字符串。
- `readCsvFile(path)// CsvInputFormat`-解析以逗号（或其他字符）分隔的字段的文件。返回元组，case class对象或POJO的数据集。支持基本的Java类型及其与Value相对应的字段类型。
- `readFileOfPrimitives(path, delimiter)`// PrimitiveInputFormat-解析以换行符（或其他char序列）定界的原始数据类型的文件，例如String或Integer使用给定的定界符。
- `readSequenceFile(Key, Value, path)`// SequenceFileInputFormat-创建JobConf并从指定的路径中读取类型为SequenceFileInputFormat，Key类和Value类的文件，并将它们作为Tuple2 <Key，Value>返回。

## 基于集合：
- `fromCollection(Iterable)`-从Iterable创建数据集。Iterable 返回的所有元素都必须是同一类型。
- `fromCollection(Iterator)`-从迭代器创建数据集。该类指定迭代器返回的元素的数据类型。
- `fromElements(elements: _*)`-从给定的对象序列创建数据集。所有对象必须具有相同的类型。
- `fromParallelCollection(SplittableIterator)`-从迭代器并行创建数据集。该类指定迭代器返回的元素的数据类型。
- `generateSequence(from, to)` -并行生成给定间隔中的数字序列。

## 通用：
- `readFile(inputFormat, path)// FileInputFormat`-接受文件输入格式。
- `createInput(inputFormat)// InputFormat`-接受通用输入格式。

# 程序数据输出（Data Sinks）
- `writeAsText()`// TextOutputFormat-将元素按行写为字符串。通过调用每个元素的 toString（）方法获得字符串。
- `writeAsCsv(...)`// CsvOutputFormat-将元组写为逗号分隔的值文件。行和字段定界符是可配置的。每个字段的值来自对象的 toString（）方法。
- `print()`// printToErr()- 在标准输出/标准错误流上打印每个元素的 toString（）值。
- `write()`// FileOutputFormat-自定义文件输出的方法和基类。支持自定义对象到字节的转换。
- `output()`// OutputFormat-最通用的输出方法，用于不基于文件的数据接收器（例如将结果存储在数据库中）。

# 转换操作（Transformations）
- Map 
- FlatMap
- MapPartition
- Filter
- Projection of Tuple DataSet
- Transformations on Grouped DataSet
- Reduce on Grouped DataSet
- GroupReduce on Grouped DataSet
- GroupCombine on a Grouped DataSet
- Aggregate on Grouped Tuple DataSet
- MinBy / MaxBy on Grouped Tuple DataSet
- Reduce on full DataSet
- GroupReduce on full DataSet
- GroupCombine on a full DataSet
- Aggregate on full Tuple DataSet
- MinBy / MaxBy on full Tuple DataSet
- Distinct
- Join
- OuterJoin
- Cross
- CoGroup
- Union
- Rebalance
- Hash-Partition
- Range-Partition
- Sort Partition
- First-n

# Connectors